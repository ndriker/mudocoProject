{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ecf658-418c-4184-8cae-e646de579a5b",
   "metadata": {},
   "source": [
    "Noah Driker, Jason Tran, Luke Ren, Farbod Ghiasi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e51abf-3aaf-46eb-b5b4-fd625b0be6b1",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02a9a2-0bca-4a24-8c7b-301261dc5aeb",
   "metadata": {},
   "source": [
    "The aim of this project is to compare the quantitative results of the performance of reinforcement learning and genetic algorithm optimization techniques on the problem of teaching a robot to walk. These methods will be used to maximize the total distance that the robot can travel in a finite number of steps. By comparing the results of these two methods, one can conclude which optimization algorithm would be more ideal in optimizing similar problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a5a97-3aa2-4517-8094-ca7032d72bfd",
   "metadata": {},
   "source": [
    "# Previous Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63df616-06f0-47d4-93b4-9f90d9c343dd",
   "metadata": {},
   "source": [
    "To define the environment in which these optimization methods will be tested on, this project uses the Ant-v2 and Humanoid-v2 enviornments from the physics engine MuJoCo provided by the OpenAI Gym toolkit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70131a5-62e1-44b8-b0ee-f0d25cbc1cde",
   "metadata": {},
   "source": [
    "The approach to reinforcement learning that is taken to maximize the distance travelled by these robots is Proximal Policy Optimization, and the orignal algorithm was described in the paper, *Proximal Policy Optimization Algorithms* by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. The paper details the algorithms use of a clipped surrogate objective function and an adaptive penalty coefficient to provide strong performance on continuous domain optimization problems. For this project, an implementation for PPO from the open-source library, RLlib, is integrated and tuned to the specific environment that was chosen for this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820816d1-6b35-4a9f-9c88-2e118b5a8c39",
   "metadata": {},
   "source": [
    "The genetic algorithm that was implemented in this project took influence from the book, *Algorithms for Optimization* by Mykel J. Kochenderfer and Tim A. Wheeler, which outlines the different phases for the optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f3f85-dadd-47e1-a064-a697d00d6f83",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9b420-1ebe-46eb-a198-ba5a0bde0778",
   "metadata": {},
   "source": [
    "The project was divided into three portions. Jason Tran worked on integrating the PPO algorithm defined in the open-source library, RLlib, with the physics environments defined in MuJoCo, which included defining an objective function and logging quantitative metrics. Noah Driker and Farbod Ghiasi worked on designing and tuning the genetic algorithm to optimize the same environments in MuJoCo. Luke Ren worked on writing and preparing the report, as well as organizing meetings and synthesizing discussions on the implementations of the algorithms. The Python packages, gym and mujoco-py, contained all of the definitions for the physics environments used to test the optimization methods. The Python package, ray, contained the code for the open-source library RLlib, used to implement the proximal policy optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16ab0c-6ff5-41eb-b255-e4690a016bc5",
   "metadata": {},
   "source": [
    "# Coding Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc804f6-907e-4c51-bc45-7d3e7b837d7d",
   "metadata": {},
   "source": [
    "# Quantitative Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9b4a6c-fcb6-41f2-a96e-eadeb86fb8ae",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3387b-c03c-4d4d-ae57-016b4ea7f6e9",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea23948-076e-4694-82d5-5a21098fb120",
   "metadata": {},
   "source": [
    "Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b9149-4902-48bc-81b1-a40652348980",
   "metadata": {},
   "source": [
    "Kochenderfer, M. J., & Wheeler, T. A. (2019). Algorithms for optimization. Mit Press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f96783-5c01-47e5-a438-cd9ff72eacc8",
   "metadata": {},
   "source": [
    "Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Goldberg, K., ... & Stoica, I. (2018, July). RLlib: Abstractions for distributed reinforcement learning. In International Conference on Machine Learning (pp. 3053-3062). PMLR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1580170-fd25-41a0-afa3-d88a1b4506fd",
   "metadata": {},
   "source": [
    "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f55a50-59a9-4e75-b4ed-86a54b3067a7",
   "metadata": {},
   "source": [
    "Todorov, E., Erez, T., & Tassa, Y. (2012, October). Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (pp. 5026-5033). IEEE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450a6b0d-ed77-498b-a7b8-c9e7911b8280",
   "metadata": {},
   "source": [
    "Noah Driker, Jason Tran, Luke Ren, Farbod Ghiasi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
